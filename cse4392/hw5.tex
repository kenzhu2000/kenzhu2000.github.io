%----PrefaceImport----%
\documentclass{article}
\usepackage{fancyhdr}
\usepackage[a4paper,margin=1in,headsep=25pt]{geometry}
\usepackage{lipsum,hyperref}
\usepackage{enumerate,fullpage,proof}
\usepackage[fontsize=12pt]{fontsize}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{listings}
\usepackage[usenames]{xcolor}
\usepackage{tcolorbox}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}

\pagestyle{fancy}
\pagenumbering{Alph}
\setlength{\headheight}{36.0pt}
\headsep = 25pt
\fancyhf{}
\lhead{CSE 4392 Special Topic: Natural Language Processing}
\rhead{Homework 5: Word Embeddings}
\lfoot{2025 Kenny Zhu & Essam Abdelghany}
\rfoot{Plagiarism will not be tolerated. We are here to help.}


% New command for blank spaces after questions
\newcommand{\answerbox}{
    \vspace{7cm} % Adjust the space size as needed
}
\newcommand{\answerboxbig}{
    \vspace{20cm} % Adjust the space size as needed
}
\newcommand{\answerboxsmall}{
    \vspace{3cm} % Adjust the space size as needed
}


%----Documentation----%
\begin{document}

\title{CSE 4392 Special Topic: Natural Language Processing}
\author{Homework 5 - Spring 2025}
\date{Due Date: Feb 24, 2025, 11:59 p.m. Central Time}
\maketitle
\thispagestyle{fancy}

%----Homeworks----%


\section*{Problem 1 - 30\%}
Derive the gradient of the negative sampling objective function with respect to the parameters ($\mathbf{u}_t$ and $\mathbf{v}_t$) used in Skip-Gram Word2Vec.
$$y=-log(\sigma(\mathbf{u}_t \cdot \mathbf{v}_c)) - \sum_{i=1}^{K}\mathbb{E}_{j\sim P(w)}log(\sigma(-\mathbf{u}_t \cdot \mathbf{v}_j))$$
Any context word vector $v_t$ might be the actual context vector $t=c$ or one of those negatively sampled $t=j$ (where $c \neq j$) or none of these. The gradient w.r.t $v_t$ should be derived for each of these three cases.
\\ \\
Before solving the problem, write all the differentiation rules you expect to use except for the most trival ones and solve in latex or clear handwriting.

\answerboxbig

\section*{Problem 2 - 70\%}
In this problem, you are required to re-implement feature extraction for the E-commerce dataset, but this time utilize advanced word embedding techniques instead of traditional Bag of Words with Logistic Linear Regression. Measure the impact on performance metric.
\begin{itemize}
    \item For each of Word2Vec, GloVe, and FastText, you will define function(s) in a new file \textbf{embeddings.py} to be able to extract the embeddings of given word(s) for any of the methods above.
    \item Moreover, you will define another function that given a document or paragraph of text, returns a single dense vector representation by pooling its word vectors 
    \item Define a function that computes a bag-of-words vector given any document
    \item By this, including your feature extraction method from last time, there are five feature extraction methods in total.
    \item Ensure that the your feature vectors only depend on the training set or are pre-trained.
    \item Make a table to compare the five methods together in terms of accuracy, F1 score, precision and recall (macro scores).
    \item Provide insights into the comparison and your justification to their performance ranking.
    \item Submit your Python files and PDF analogues as usual. No zipping required.
    \item As a bonus you can perform nonlinear dimensionality reduction and visualize the different features
\end{itemize}

\end{document}
