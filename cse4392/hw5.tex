%----PrefaceImport----%
\documentclass{article}
\usepackage{fancyhdr}
\usepackage[a4paper,margin=1in,headsep=25pt]{geometry}
\usepackage{lipsum,hyperref}
\usepackage{enumerate,fullpage,proof}
\usepackage[fontsize=12pt]{fontsize}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{listings}
\usepackage[usenames]{xcolor}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}

\pagestyle{fancy}
\pagenumbering{Alph}
\setlength{\headheight}{36.0pt}
\headsep = 25pt
\fancyhf{}
\lhead{CSE 4392/5369}
\rhead{Homework 5: Word Embedding}
\lfoot{2026 Kenny Zhu}
%----Documentation----%
\begin{document}

\title{CSE 4392/5369 Special Topic: Natural Language Processing}
\author{Homework 5 - Spring 2026}
\date{Due Date: Feb 23, 2026, 11:59 PM}
\maketitle
\thispagestyle{fancy}

%----Homeworks----%

In this lecture, we learned how to embed words into a feature space of specific dimensions to densely represent a word. The core idea is that words with similar semantics or grammar will be mapped to vectors that are close in distance. We will further reinforce word embedding through three exercises and apply it in practical applications.

\section*{Problem 1 - 30\%}
In this problem, you will work on deriving the gradient for the negative sampling objective function used in Skip-Gram Word2Vec. The negative sampling approach is a technique to efficiently train word embeddings by sampling negative examples.
Please refer to \textbf{Slides 28} and derive the gradient of the negative sampling objective function.
$$y=-log(\sigma(\mathbf{u}_t \cdot \mathbf{v}_c)) - \sum_{i=1}^{K}\mathbb{E}_{j\sim P(w)}log(\sigma(-\mathbf{u}_t \cdot \mathbf{v}_j))$$

\section*{Problem 2 - 40\%}
In this problem, you are required to derive the gradient expression shown below and show all steps.
Please refer to \textbf{Slides 25} for the corresponding derivation steps and notations..

\[
\frac{\partial y}{\partial \mathbf{v}_k}
=
-\mathbf{1}(k=c)\mathbf{u}_t + P(k \mid t)\mathbf{u}_t
\]


\section*{Problem 3 - 30\%}
In this problem, you are required to re-implement feature extraction for the E-commerce dataset, but this time utilize advanced word embedding techniques instead of traditional Bag of Words with Logistic Linear Regression. Measure the impact on accuracy.
\begin{itemize}
    \item Choose a modern word embedding technique for feature extraction. Options include Word2Vec, GloVe, or FastText. Select one that aligns with the dataset's characteristics and size.
    \item Utilize the \textbf{pretrained} chosen word embedding to convert words in the dataset into dense vectors. Ensure that the vectors capture semantic relationships between words.
    \item Train the model on a subset of the dataset and evaluate its performance using appropriate metrics (e.g., accuracy, precision, recall). Compare the results with the previous Bag of Words approach.
    \item Provide insights into how the choice of word embedding technique affected the model's accuracy compared to the Bag of Words approach. Discuss any observed improvements or challenges.
    \item \textbf{Attach your codes}
\end{itemize}

\vspace{0.8em}
\noindent{\color{red}\footnotesize
\textbf{Submission Format:}
Submit one zip file via Canvas containing only
the \texttt{.pdf} version of your homework (typed submissions are preferred;
scanned images must be readable), the corresponding
\texttt{.py} source files, and a \texttt{README} file describing how to run the code.
The zip file must be named
\texttt{lastname\_studentID\_hw5.zip}.
}

\end{document}
