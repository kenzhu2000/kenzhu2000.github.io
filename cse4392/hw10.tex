%----PrefaceImport----%
\documentclass{article}
\usepackage{fancyhdr}
\usepackage[a4paper,margin=1in,headsep=25pt]{geometry}
\usepackage{lipsum,hyperref}
\usepackage{enumerate,fullpage,proof}
\usepackage[fontsize=12pt]{fontsize}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{listings}
\usepackage[usenames]{xcolor}
\usepackage{tcolorbox}
\usepackage{minted}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}

\pagestyle{fancy}
\pagenumbering{Alph}
\setlength{\headheight}{36.0pt}
\headsep = 25pt
\fancyhf{}
\lhead{CSE 4392 Special Topic: Natural Language Processing}
\rhead{Homework 10: Transformers}
\lfoot{2025 Kenny Zhu & Essam Abdelghany}
\rfoot{Plagiarism will not be tolerated. We are here to help.}


% New command for blank spaces after questions
\newcommand{\answerbox}{
    \vspace{7cm} % Adjust the space size as needed
}
\newcommand{\answerboxbig}{
    \vspace{20cm} % Adjust the space size as needed
}
\newcommand{\answerboxsmall}{
    \vspace{3cm} % Adjust the space size as needed
}


%----Documentation----%
\begin{document}

\title{CSE 4392 Special Topic: Natural Language Processing}
\begin{center}
    \Large\textbf{CSE 4392 Special Topic: Natural Language Processing} \\
    \large Homework 10 - Spring 2025 \\
    \large Due: April 17th
\end{center}

\thispagestyle{fancy}


\subsection*{Question 1 - 50\%}
Implement and Test Multihead Attention from scratch with Python. You should test it by producing equivalent results to your own computation on paper or to PyTorch's built-in Multihead Attention module (feel free to set weights to identity matrices for testing only). In a separate paper or PDF, explain what each step of computation does in this multihead attention block.

Preferably, submit a PDF notebook using \hyperlink{https://htmtopdf.herokuapp.com/ipynbviewer/#google_vignette}{this tool}. Or submit a Python file and a report documenting your work and containing test outputs.


\answerboxbig

\subsection*{Question 2 - 50\%}
Revisit your code from last tutorial for fake news detection and add transformer encoder (using PyTorch) and compare it with LSTM, GRU and RNN in terms of mean inference speed over the test set and classification performance. Beware that the transformer may require more tuning and that the transformer encoder from PyTorch may not have a built-in positional encoding (it's in a different module).

Preferably, submit a PDF notebook using \hyperlink{https://htmtopdf.herokuapp.com/ipynbviewer/#google_vignette}{this tool}. Or submit a Python file and a report documenting your work.

\end{document}
