%----PrefaceImport----%
\documentclass{article}
\usepackage{fancyhdr}
\usepackage[a4paper,margin=1in,headsep=25pt]{geometry}
\usepackage{lipsum,hyperref}
\usepackage{enumerate,fullpage,proof}
\usepackage[fontsize=12pt]{fontsize}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{listings}
\usepackage[usenames]{xcolor}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}

\pagestyle{fancy}
\pagenumbering{Alph}
\setlength{\headheight}{36.0pt}
\headsep = 25pt
\fancyhf{}
\lhead{CSE 4392 Special Topic: Natural Language Processing}
\rhead{Homework 2: Language Models}
\lfoot{2024 Kenny Zhu}
%----Documentation----%
\begin{document}

\title{CSE 4392 Special Topic: Natural Language Processing}
\author{Homework 2 - Spring 2024}
\date{Due Date: Feb 3, 2024, 11:59 p.m. Central Time}
\maketitle
\thispagestyle{fancy}

%----Homeworks----%



As we delve into the fascinating world of Natural Language Processing (NLP), one of the fundamental concepts we encounter is the N-gram language model. This model is a crucial tool in understanding how sequences of words are structured in a language, and it forms the basis for many applications in NLP, such as text prediction, machine translation, and speech recognition.\\
\newline
In this assignment, you will work with some small corpora that are deliberately 
designed to be simple yet challenging. You'll train your n-gram model on this corpus and then test its effectiveness on a new sentence. This process will involve handling scenarios where your model encounters unseen data, thereby introducing you to the concept of smoothing - a critical aspect in the real-world application of language models.\\

\section*{Problem 1 - 40\%}
The corpus provided for training the bi-gram model consists of the following four sentences:

\begin{itemize}
    \item Cats chase after playful mice
    \item Birds sing at morning dawn
    \item Fish swim in the pond
    \item Dogs bark at passing cars
\end{itemize}

The corpus provided for testing the bi-gram model consists of the following two sentences:

\begin{itemize}
    \item Cats sing and dogs swim
    \item Playful dogs chase birds at dawn
\end{itemize}

\newpage

\subsection*{Question 1 - 15\%}
Please write down the raw counts in the training data for the following 
bigrams found in \textbf{Test Sentence 1}\\
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
     & cats & sing & and & dog & swim \\ \hline
    cats &  & &&& \\ \hline
    sing &  & &&& \\ \hline
    and &  & &&& \\ \hline
    dogs &  & &&& \\ \hline
    swim &  & &&& \\ \hline
    \end{tabular}
\end{center}


\subsection*{Question 2 - 15\%}
Upon reviewing the table, you'll notice several zeros indicating absent bigrams from our training data. To overcome this, please apply Add-one Laplace Smoothing to filling the following table the reconstituted counts. This adjustment 
will help us better understand our model's performance.\\
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
     & cats & sing & and & dog & swim \\ \hline
    cats &  & &&& \\ \hline
    sing &  & &&& \\ \hline
    and &  & &&& \\ \hline
    dogs &  & &&& \\ \hline
    swim &  & &&& \\ \hline
    \end{tabular}
\end{center}
\subsection*{Question 3 - 10\%}
Please calculate the PPL of the \textbf{Whole Test Set} after Laplace Smoothing .

\section*{Problem 2 - 60\%}
The corpus provided for training the trigram model consists of the 
following sentences:

\begin{itemize}
    \item the cat watched children play in the park.
    \item their laughter echoed near the fragrant garden.
    \item the breeze spread the garden's scent into the city.
    \item it wafted past the cafe, famous for apple pie.
    \item the cafe's aroma reminded people of the nearby library.
    \item the library held tales of the ancient clock tower.
    \item the tower tolled, echoing in the quiet morning streets.
    \item these streets, bustling by day, were peaceful at dawn.
    \item at night, they lay under a star-filled sky.
    \item the moonlight shone on the lake where a fisherman waited.
\end{itemize}

The corpus provided for testing the trigram model consists of the following 
three sentences:

\begin{itemize}
    \item the sunset painted the city sky with colors.
    \item an old train's whistle echoed past the lake.
    \item soft music played in the cozy cafe.
\end{itemize}

Note that punctuation and 's are treated as separate tokens.
In the last problem, we discussed the concept of bigrams. Now, let's shift our focus to trigrams and delve into their practical applications in programming.

\subsection*{Question 1 - 40\%}
Create a trigram model using the given corpus. Calculate the probabilities 
of each trigram in the test set based on your model.
If a trigram from the test sentence does not exist in your training data, handle this scenario using \textbf{Linear Interpolation} smoothing technique.
You can set the $\lambda_1$ as 0.5, $\lambda_2$ as 0.4, $\lambda_3$ as 0.1 for initialization. (You don't need to fine-tune the parameter.) Please take a screenshot of the experimental results and attach it in the PDF.

\subsection*{Question 2 - 20\%}
Calculate the perplexity of the \textbf{whole test set}. 
Please take a screenshot of the experimental results and attach it in the PDF.

\subsection*{}

{\color{red}Please finish this homework in Python3.}\\
{\color{red}Please add a readme.md to introduce how to run the code.}\\
{\color{red}Please upload a zip file contains the pdf and .py file.}

\end{document}
